{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ycFla-uCyShrYcgsSZp6fifFJFcF5Rvl",
      "authorship_tag": "ABX9TyO6WpupEkLjl1GnllJONLwp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PimonwunPhoomsrikaew/Prediction_Stroke/blob/main/Prediction_Stroke.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxgmVFcOKyOg"
      },
      "source": [
        "#INSTALL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxTfq0aobE3c"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh4sFk7jbE3c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8PAbJaiK5GM"
      },
      "source": [
        "#Import Lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndfBi3ffbLIy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import mne\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, classification_report, ConfusionMatrixDisplay\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO4yELYQQood"
      },
      "source": [
        "#Define the filter frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGm3QFp5MSbX"
      },
      "outputs": [],
      "source": [
        "# Define the filter frequencies\n",
        "highpass_freq = 1  # Hz\n",
        "lowpass_freq = 35  # Hz\n",
        "sampling_rate = 256  # Hz\n",
        "\n",
        "channel_name = {'EEG Fp1-Cz': 'Fp1', 'EEG Fp2-Cz': 'Fp2', 'EEG F7-Cz': 'F7', 'EEG F8-Cz': 'F8', 'EEG T3-Cz': 'T3', 'EEG T4-Cz': 'T4',  'EEG T5-Cz': 'T5', 'EEG T6-Cz': 'T6', 'EEG O1-Cz': 'O1', 'EEG O2-Cz': 'O2', 'EEG F3-Cz': 'F3', 'EEG F4-Cz': 'F4', 'EEG C3-Cz': 'C3', 'EEG C4-Cz': 'C4', 'EEG P3-Cz': 'P3', 'EEG P4-Cz': 'P4'}\n",
        "channel_dropout = ['E1/Pg1-Cz', 'E2/Pg2-Cz','EEG Fz-Cz', 'EEG Pz-Cz', 'T1-Cz', 'T2-Cz', 'EEG A1-Cz', 'EEG A2-Cz', 'EEG Cz-Cz', '1A-1R', '2A-2R', '3A-3R', '4A-4R', '5A-5R', '6A-6R', '7A-7R', 'Status']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9Y6dgZMQsic"
      },
      "source": [
        "#ICA Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N88YV-MvMbcS"
      },
      "outputs": [],
      "source": [
        "def ica_process(raw_data):\n",
        "    raw_data.apply_function(lambda x: x - x.mean(), picks='eeg', channel_wise=True)\n",
        "    eeg_data = raw_data.get_data().T\n",
        "    ica = FastICA(n_components=eeg_data.shape[1], random_state=42)\n",
        "    components = ica.fit_transform(eeg_data)\n",
        "    components_to_remove = [0, 1, 5, 6, 10]\n",
        "    for comp in components_to_remove:\n",
        "        components[:, comp] = 0\n",
        "    restored = ica.inverse_transform(components)\n",
        "    return restored\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmoDtJuVQy4O"
      },
      "source": [
        "#Preprocess Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdA34y8OMd3U"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(file_name, group):\n",
        "    print('-----Preprocess Data-----')\n",
        "    iir_params = dict(order=5, ftype='butter')\n",
        "    data = mne.io.read_raw_edf(file_name, preload=True)\n",
        "    data.drop_channels([ch for ch in data.ch_names if any(dc in ch for dc in channel_dropout)])\n",
        "    max_time = data.times[-1]\n",
        "    data.crop(0, min(1800, max_time))\n",
        "    data.rename_channels({ch: channel_name.get(ch, channel_name.get(ch.replace('EEG ', ''), ch)) for ch in data.ch_names})\n",
        "    data.resample(sampling_rate)\n",
        "    data.apply_function(lambda x: x * 1e6, picks='all', channel_wise=True)\n",
        "    data.notch_filter(50, method='iir', iir_params=iir_params)\n",
        "    data.filter(highpass_freq, lowpass_freq, method='iir', iir_params=iir_params)\n",
        "\n",
        "    # restored_data = ica_process(data)  # Move ICA processing\n",
        "\n",
        "    # Specify bipolar electrode pairs (customize this as per your EEG channels)\n",
        "    anode  = ['Fp1', 'F7', 'T3', 'T5', 'Fp2', 'F8', 'T4', 'T6','Fp1', 'F3', 'C3', 'P3','Fp2', 'F4', 'C4', 'P4']\n",
        "    cathode = ['F7', 'T3', 'T5', 'O1', 'F8', 'T4', 'T6', 'O2','F3', 'C3', 'P3', 'O1', 'F4', 'C4', 'P4', 'O2']\n",
        "\n",
        "    # Create bipolar reference\n",
        "    restored_data = mne.set_bipolar_reference(data, anode=anode, cathode=cathode)\n",
        "    print('-----restores_data-----' + str(restored_data))\n",
        "    print('-----restores_data info-----' + str(restored_data.info))\n",
        "    file_base_name = os.path.basename(file_name)\n",
        "    file_id = os.path.splitext(file_base_name)[0]\n",
        "    return restored_data, restored_data.info, file_id, group\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht8ALFP3N38i"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess all data\n",
        "def preprocess_all_data():\n",
        "    hc_files = glob.glob(\"/content/drive/MyDrive/EEG_PROJECT/EEG_ANALYSIS/data_set_Cz/HC/*.edf\")\n",
        "    st_files = glob.glob(\"/content/drive/MyDrive/EEG_PROJECT/EEG_ANALYSIS/data_set_Cz/ST/*.edf\")\n",
        "\n",
        "    hc_preprocessed = [preprocess_data(file, 0) for file in hc_files]  # Group = 0 for HC\n",
        "    st_preprocessed = [preprocess_data(file, 1) for file in st_files]  # Group = 1 for ST\n",
        "\n",
        "    return hc_preprocessed, st_preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NZ1vz8ITOMVb"
      },
      "outputs": [],
      "source": [
        "hc_preprocessed, st_preprocessed = preprocess_all_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTal4hwA6fDN"
      },
      "outputs": [],
      "source": [
        "print(len(hc_preprocessed))\n",
        "print(len(st_preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-K2wt4J-b-1"
      },
      "source": [
        "##Difference Feature Power and QEEG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGQuoFGyMiR-"
      },
      "outputs": [],
      "source": [
        "def extract_features(restored_data, info, file_id, group):\n",
        "    print('-----Extract Feature -----')\n",
        "     # Get the data from restored_data using get_data()\n",
        "    restored_array = restored_data.get_data().T\n",
        "\n",
        "    restored_raw = mne.io.RawArray(restored_array, info)\n",
        "    spectrum = restored_raw.compute_psd(method='welch', fmin=0.5, fmax=30, n_fft=800)\n",
        "\n",
        "    psd, freqs = spectrum.get_data(return_freqs=True)\n",
        "    psd_normal = psd / np.mean(psd, axis=-1, keepdims=True)\n",
        "\n",
        "    frequency_bands = {\n",
        "        'Delta': (0.5, 4),\n",
        "        'Theta': (4, 8),\n",
        "        'Alpha': (8, 12),\n",
        "        'Beta': (12, 30),\n",
        "        'Gamma': (30, 40)\n",
        "    }\n",
        "\n",
        "    print('-----band_power-----')\n",
        "    def band_power(psd, freqs, band):\n",
        "        band_freqs = np.logical_and(freqs >= band[0], freqs <= band[1])\n",
        "        return np.mean(psd[:, band_freqs], axis=1)\n",
        "\n",
        "    band_powers = {band: band_power(psd_normal, freqs, frequency_bands[band]) for band in frequency_bands}\n",
        "\n",
        "    print('-----Compute QEEG features-----')\n",
        "    dar = band_powers['Delta'] / band_powers['Alpha']\n",
        "    dtr = band_powers['Delta'] / band_powers['Theta']\n",
        "    dtar = (band_powers['Delta'] + band_powers['Theta']) / band_powers['Alpha']\n",
        "    tar = band_powers['Theta'] / band_powers['Alpha']\n",
        "    tbr = band_powers['Theta'] / band_powers['Beta']\n",
        "    tbar = band_powers['Theta'] / (band_powers['Alpha'] + band_powers['Beta'])\n",
        "    print('-----Calculate Difference-----')\n",
        "\n",
        "    delta_power_diff_Fp1_Fp2 = band_powers['Delta'][0] - band_powers['Delta'][1]\n",
        "    theta_power_diff_Fp1_Fp2 = band_powers['Theta'][0] - band_powers['Theta'][1]\n",
        "    alpha_power_diff_Fp1_Fp2 = band_powers['Alpha'][0] - band_powers['Alpha'][1]\n",
        "    dar_Fp1_Fp2 = dar[0] - dar[1]\n",
        "    dtr_Fp1_Fp2 = dtr[0] - dtr[1]\n",
        "    dtar_Fp1_Fp2 = dtar[0] - dtar[1]\n",
        "\n",
        "    delta_power_diff_F3_F4 = band_powers['Delta'][3] - band_powers['Delta'][5]\n",
        "    theta_power_diff_F3_F4 = band_powers['Theta'][3] - band_powers['Theta'][5]\n",
        "    alpha_power_diff_F3_F4 = band_powers['Alpha'][3] - band_powers['Alpha'][5]\n",
        "    dar_F3_F4 = dar[3] - dar[5]\n",
        "    dtr_F3_F4 = dtr[3] - dtr[5]\n",
        "    dtar_F3_F4 = dtar[3] - dtar[5]\n",
        "\n",
        "    delta_power_diff_F7_F8 = band_powers['Delta'][2] - band_powers['Delta'][6]\n",
        "    theta_power_diff_F7_F8 = band_powers['Theta'][2] - band_powers['Theta'][6]\n",
        "    alpha_power_diff_F7_F8 = band_powers['Alpha'][2] - band_powers['Alpha'][6]\n",
        "    dar_F7_F8 = dar[2] - dar[6]\n",
        "    dtr_F7_F8 = dtr[2] - dtr[6]\n",
        "    dtar_F7_F8 = dtar[2] - dtar[6]\n",
        "\n",
        "    delta_power_diff_A1_A2 = band_powers['Delta'][7] - band_powers['Delta'][12]\n",
        "    theta_power_diff_A1_A2 = band_powers['Theta'][7] - band_powers['Theta'][12]\n",
        "    alpha_power_diff_A1_A2 = band_powers['Alpha'][7] - band_powers['Alpha'][12]\n",
        "    dar_A1_A2 = dar[7] - dar[12]\n",
        "    dtr_A1_A2 = dtr[7] - dtr[12]\n",
        "    dtar_A1_A2 = dtar[7] - dtar[12]\n",
        "\n",
        "    delta_power_diff_T3_T4 = band_powers['Delta'][8] - band_powers['Delta'][11]\n",
        "    theta_power_diff_T3_T4 = band_powers['Theta'][8] - band_powers['Theta'][11]\n",
        "    alpha_power_diff_T3_T4 = band_powers['Alpha'][8] - band_powers['Alpha'][11]\n",
        "    dar_T3_T4 = dar[8] - dar[11]\n",
        "    dtr_T3_T4 = dtr[8] - dtr[11]\n",
        "    dtar_T3_T4 = dtar[8] - dtar[11]\n",
        "\n",
        "    delta_power_diff_T5_T6 = band_powers['Delta'][13] - band_powers['Delta'][17]\n",
        "    theta_power_diff_T5_T6 = band_powers['Theta'][13] - band_powers['Theta'][17]\n",
        "    alpha_power_diff_T5_T6 = band_powers['Alpha'][13] - band_powers['Alpha'][17]\n",
        "    dar_T5_T6 = dar[13] - dar[17]\n",
        "    dtr_T5_T6 = dtr[13] - dtr[17]\n",
        "    dtar_T5_T6 = dtar[13] - dtar[17]\n",
        "\n",
        "    delta_power_diff_P3_P4 = band_powers['Delta'][14] - band_powers['Delta'][16]\n",
        "    theta_power_diff_P3_P4 = band_powers['Theta'][14] - band_powers['Theta'][16]\n",
        "    alpha_power_diff_P3_P4 = band_powers['Alpha'][14] - band_powers['Alpha'][16]\n",
        "    dar_P3_P4 = dar[14] - dar[16]\n",
        "    dtr_P3_P4 = dtr[14] - dtr[16]\n",
        "    dtar_P3_P4 = dtar[14] - dtar[16]\n",
        "\n",
        "    delta_power_diff_O1_O2 = band_powers['Delta'][18] - band_powers['Delta'][19]\n",
        "    theta_power_diff_O1_O2 = band_powers['Theta'][18] - band_powers['Theta'][19]\n",
        "    alpha_power_diff_O1_O2 = band_powers['Alpha'][18] - band_powers['Alpha'][19]\n",
        "    dar_O1_O2 = dar[18] - dar[19]\n",
        "    dtr_O1_O2 = dtr[18] - dtr[19]\n",
        "    dtar_O1_O2 = dtar[18] - dtar[19]\n",
        "\n",
        "    print('-----features-----')\n",
        "    features = {\n",
        "        'id': file_id,\n",
        "\n",
        "        'delta_power_diff_Fp1_Fp2': delta_power_diff_Fp1_Fp2,\n",
        "        'theta_power_diff_Fp1_Fp2': theta_power_diff_Fp1_Fp2,\n",
        "        'alpha_power_diff_Fp1_Fp2': alpha_power_diff_Fp1_Fp2,\n",
        "        'dar_Fp1_Fp2': dar_Fp1_Fp2,\n",
        "        'dtr_Fp1_Fp2': dtr_Fp1_Fp2,\n",
        "        'dtar_Fp1_Fp2': dtar_Fp1_Fp2,\n",
        "\n",
        "        'delta_power_diff_F3_F4': delta_power_diff_F3_F4,\n",
        "        'theta_power_diff_F3_F4': theta_power_diff_F3_F4,\n",
        "        'alpha_power_diff_F3_F4': alpha_power_diff_F3_F4,\n",
        "        'dar_F3_F4': dar_F3_F4,\n",
        "        'dtr_F3_F4': dtr_F3_F4,\n",
        "        'dtar_F3_F4': dtar_F3_F4,\n",
        "\n",
        "        'delta_power_diff_F7_F8': delta_power_diff_F7_F8,\n",
        "        'theta_power_diff_F7_F8': theta_power_diff_F7_F8,\n",
        "        'alpha_power_diff_F7_F8': alpha_power_diff_F7_F8,\n",
        "        'dar_F7_F8': dar_F7_F8,\n",
        "        'dtr_F7_F8': dtr_F7_F8,\n",
        "        'dtar_F7_F8': dtar_F7_F8,\n",
        "\n",
        "        'delta_power_diff_A1_A2': delta_power_diff_A1_A2,\n",
        "        'theta_power_diff_A1_A2': theta_power_diff_A1_A2,\n",
        "        'alpha_power_diff_A1_A2': alpha_power_diff_A1_A2,\n",
        "        'dar_A1_A2': dar_A1_A2,\n",
        "        'dtr_A1_A2': dtr_A1_A2,\n",
        "        'dtar_A1_A2': dtar_A1_A2,\n",
        "\n",
        "        'delta_power_diff_T3_T4': delta_power_diff_T3_T4,\n",
        "        'theta_power_diff_T3_T4': theta_power_diff_T3_T4,\n",
        "        'alpha_power_diff_T3_T4': alpha_power_diff_T3_T4,\n",
        "        'dar_T3_T4': dar_T3_T4,\n",
        "        'dtr_T3_T4': dtr_T3_T4,\n",
        "        'dtar_T3_T4': dtar_T3_T4,\n",
        "\n",
        "        'delta_power_diff_T5_T6': delta_power_diff_T5_T6,\n",
        "        'theta_power_diff_T5_T6': theta_power_diff_T5_T6,\n",
        "        'alpha_power_diff_T5_T6': alpha_power_diff_T5_T6,\n",
        "        'dar_T5_T6': dar_T5_T6,\n",
        "        'dtr_T5_T6': dtr_T5_T6,\n",
        "        'dtar_T5_T6': dtar_T5_T6,\n",
        "\n",
        "        'delta_power_diff_P3_P4': delta_power_diff_P3_P4,\n",
        "        'theta_power_diff_P3_P4': theta_power_diff_P3_P4,\n",
        "        'alpha_power_diff_P3_P4': alpha_power_diff_P3_P4,\n",
        "        'dar_P3_P4': dar_P3_P4,\n",
        "        'dtr_P3_P4': dtr_P3_P4,\n",
        "        'dtar_P3_P4': dtar_P3_P4,\n",
        "\n",
        "        'delta_power_diff_O1_O2': delta_power_diff_O1_O2,\n",
        "        'theta_power_diff_O1_O2': theta_power_diff_O1_O2,\n",
        "        'alpha_power_diff_O1_O2': alpha_power_diff_O1_O2,\n",
        "        'dar_O1_O2': dar_O1_O2,\n",
        "        'dtr_O1_O2': dtr_O1_O2,\n",
        "        'dtar_O1_O2': dtar_O1_O2,\n",
        "\n",
        "        'Group': group\n",
        "    }\n",
        "\n",
        "    features_df = pd.DataFrame([features], index=[file_id])\n",
        "    # display(features_df)\n",
        "    return features_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPPUz2lMQ-91"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aLGxCZNNPej"
      },
      "outputs": [],
      "source": [
        "def load_data(hc_preprocessed, st_preprocessed):\n",
        "    data = []\n",
        "\n",
        "    for restored_data, info, file_id, group in hc_preprocessed:\n",
        "        print(hc_preprocessed[0])\n",
        "        print(st_preprocessed[0])\n",
        "        print(info[\"ch_names\"])\n",
        "        features_df = extract_features(restored_data, info, file_id, group)\n",
        "        data.append(features_df)\n",
        "\n",
        "    for restored_data, info, file_id, group in st_preprocessed:\n",
        "        features_df = extract_features(restored_data, info, file_id, group)\n",
        "        data.append(features_df)\n",
        "\n",
        "    combined_df = pd.concat(data, ignore_index=True)\n",
        "\n",
        "    # save DataFrame to CSV\n",
        "    # combined_df.to_csv('output_file.csv', index=False)\n",
        "\n",
        "    display(combined_df)\n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgMxJm5DMvDB"
      },
      "outputs": [],
      "source": [
        "Data_Frame  = load_data(hc_preprocessed, st_preprocessed)\n",
        "print('Data_Frame = ', Data_Frame.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#plot Distribution of Feature"
      ],
      "metadata": {
        "id": "n_nPT7DO3fAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming Data_Frame contains a 'Group' column indicating 'HC' and 'ST' groups\n",
        "Data_Frame['Group_Label'] = Data_Frame['Group'].map({0: 'HC', 1: 'ST'})\n",
        "\n",
        "features = [\n",
        "    'delta_power_diff_Fp1_Fp2', 'theta_power_diff_Fp1_Fp2', 'alpha_power_diff_Fp1_Fp2',\n",
        "    'dar_Fp1_Fp2', 'dtr_Fp1_Fp2', 'dtar_Fp1_Fp2',\n",
        "    'delta_power_diff_F3_F4', 'theta_power_diff_F3_F4', 'alpha_power_diff_F3_F4',\n",
        "    'dar_F3_F4', 'dtr_F3_F4', 'dtar_F3_F4',\n",
        "    'delta_power_diff_F7_F8', 'theta_power_diff_F7_F8', 'alpha_power_diff_F7_F8',\n",
        "    'dar_F7_F8', 'dtr_F7_F8', 'dtar_F7_F8',\n",
        "    'delta_power_diff_A1_A2', 'theta_power_diff_A1_A2', 'alpha_power_diff_A1_A2',\n",
        "    'dar_A1_A2', 'dtr_A1_A2', 'dtar_A1_A2',\n",
        "    'delta_power_diff_T3_T4', 'theta_power_diff_T3_T4', 'alpha_power_diff_T3_T4',\n",
        "    'dar_T3_T4', 'dtr_T3_T4', 'dtar_T3_T4',\n",
        "    'delta_power_diff_T5_T6', 'theta_power_diff_T5_T6', 'alpha_power_diff_T5_T6',\n",
        "    'dar_T5_T6', 'dtr_T5_T6', 'dtar_T5_T6',\n",
        "    'delta_power_diff_P3_P4', 'theta_power_diff_P3_P4', 'alpha_power_diff_P3_P4',\n",
        "    'dar_P3_P4', 'dtr_P3_P4', 'dtar_P3_P4',\n",
        "    'delta_power_diff_O1_O2', 'theta_power_diff_O1_O2', 'alpha_power_diff_O1_O2',\n",
        "    'dar_O1_O2', 'dtr_O1_O2', 'dtar_O1_O2'\n",
        "]\n",
        "\n",
        "palette = {'HC': 'blue', 'ST': 'red'}\n",
        "\n",
        "# Plot boxplot of each feature for both groups\n",
        "for feature in features:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(data=Data_Frame, x='Group_Label', y=feature, palette=palette)\n",
        "    plt.title(f'{feature}')\n",
        "    plt.xlabel('Group')\n",
        "    plt.ylabel(feature)\n",
        "    plt.savefig(f'{feature}_boxplot.png')\n",
        "    plt.show()\n",
        "\n",
        "# Create a figure and a grid of subplots\n",
        "fig, axes = plt.subplots(8, 6, figsize=(24, 32))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each feature in the corresponding subplot\n",
        "for i, feature in enumerate(features):\n",
        "    sns.boxplot(data=Data_Frame, x='Group_Label', y=feature, palette=palette, ax=axes[i])\n",
        "    axes[i].set_title(f'{feature}')\n",
        "    axes[i].set_xlabel('')\n",
        "    axes[i].set_ylabel('')\n",
        "\n",
        "# Remove empty subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.savefig('all_features_boxplot.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4E7USyFs3hYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# P-value"
      ],
      "metadata": {
        "id": "iFBqOiFyGmr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# Assuming Data_Frame contains the features and a 'Group' column\n",
        "features = [\n",
        "    'delta_power_diff_Fp1_Fp2', 'theta_power_diff_Fp1_Fp2', 'alpha_power_diff_Fp1_Fp2',\n",
        "    'dar_Fp1_Fp2', 'dtr_Fp1_Fp2', 'dtar_Fp1_Fp2',\n",
        "    'delta_power_diff_F3_F4', 'theta_power_diff_F3_F4', 'alpha_power_diff_F3_F4',\n",
        "    'dar_F3_F4', 'dtr_F3_F4', 'dtar_F3_F4',\n",
        "    'delta_power_diff_F7_F8', 'theta_power_diff_F7_F8', 'alpha_power_diff_F7_F8',\n",
        "    'dar_F7_F8', 'dtr_F7_F8', 'dtar_F7_F8',\n",
        "    'delta_power_diff_A1_A2', 'theta_power_diff_A1_A2', 'alpha_power_diff_A1_A2',\n",
        "    'dar_A1_A2', 'dtr_A1_A2', 'dtar_A1_A2',\n",
        "    'delta_power_diff_T3_T4', 'theta_power_diff_T3_T4', 'alpha_power_diff_T3_T4',\n",
        "    'dar_T3_T4', 'dtr_T3_T4', 'dtar_T3_T4',\n",
        "    'delta_power_diff_T5_T6', 'theta_power_diff_T5_T6', 'alpha_power_diff_T5_T6',\n",
        "    'dar_T5_T6', 'dtr_T5_T6', 'dtar_T5_T6',\n",
        "    'delta_power_diff_P3_P4', 'theta_power_diff_P3_P4', 'alpha_power_diff_P3_P4',\n",
        "    'dar_P3_P4', 'dtr_P3_P4', 'dtar_P3_P4',\n",
        "    'delta_power_diff_O1_O2', 'theta_power_diff_O1_O2', 'alpha_power_diff_O1_O2',\n",
        "    'dar_O1_O2', 'dtr_O1_O2', 'dtar_O1_O2'\n",
        "]\n",
        "\n",
        "# Split the data into two groups\n",
        "group_HC = Data_Frame[Data_Frame['Group'] == 0]\n",
        "group_ST = Data_Frame[Data_Frame['Group'] == 1]\n",
        "\n",
        "# Calculate mean, standard deviation, and p-values for each feature\n",
        "results = []\n",
        "\n",
        "for feature in features:\n",
        "    hc_mean = group_HC[feature].mean()\n",
        "    hc_std = group_HC[feature].std()\n",
        "    st_mean = group_ST[feature].mean()\n",
        "    st_std = group_ST[feature].std()\n",
        "    stat, p_value = mannwhitneyu(group_HC[feature], group_ST[feature], alternative='two-sided')\n",
        "    results.append({\n",
        "        'Feature': feature,\n",
        "        'HC Mean (±SD)': f'{hc_mean:.2f} (±{hc_std:.2f})',\n",
        "        'ST Mean (±SD)': f'{st_mean:.2f} (±{st_std:.2f})',\n",
        "        'p-value': f'{p_value:.3e}'\n",
        "    })\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Mark significance levels\n",
        "def annotate_p_value(row):\n",
        "    p = float(row['p-value'])\n",
        "    if p < 0.001:\n",
        "        return '<0.001***'\n",
        "    elif p < 0.01:\n",
        "        return f'{p:.3f}**'\n",
        "    elif p < 0.05:\n",
        "        return f'{p:.3f}*'\n",
        "    else:\n",
        "        return f'{p:.3f}'\n",
        "\n",
        "results_df['p-value'] = results_df.apply(annotate_p_value, axis=1)\n",
        "\n",
        "# Print the results\n",
        "print(results_df)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results_df.to_csv('feature_statistics.csv', index=False)\n"
      ],
      "metadata": {
        "id": "6YfRJD_4HWBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrgwYSKbMaBF"
      },
      "source": [
        "###PCA Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdTPWUVlL90_"
      },
      "outputs": [],
      "source": [
        "# Separate features and labels\n",
        "X = Data_Frame.drop(columns=['Group','id','Group_Label'])\n",
        "y = Data_Frame['Group']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=10)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Variance explained by each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "# print(f'Explained variance by each component: {explained_variance}')\n",
        "# print(f'Cumulative explained variance: {np.cumsum(explained_variance)}')\n",
        "\n",
        "# Plot the explained variance\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\n",
        "plt.title('Explained Variance by Principal Components')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Loadings of the original features on the principal components\n",
        "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
        "loadings_df = pd.DataFrame(loadings, index=X.columns, columns=[f'PC{i+1}' for i in range(10)])\n",
        "print(loadings_df)\n",
        "\n",
        "# Assuming loadings_df is your DataFrame with loadings\n",
        "n_plot = 1\n",
        "num_features = len(loadings_df)\n",
        "tenth_point = num_features // n_plot\n",
        "\n",
        "# Split the loadings DataFrame into ten parts\n",
        "loadings_df_parts = [loadings_df.iloc[i*tenth_point:(i+1)*tenth_point, :] for i in range(n_plot)]\n",
        "\n",
        "# Plot each tenth of the features\n",
        "for i, loadings_part in enumerate(loadings_df_parts, 1):\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    sns.heatmap(loadings_part, annot=True, cmap='coolwarm', cbar=True, annot_kws={\"size\": 10})\n",
        "    plt.title(f'Loadings of the Original Features on the Principal Components (Part {i})')\n",
        "    plt.xlabel('Principal Components')\n",
        "    plt.ylabel('Original Features')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plot PCA\n",
        "label_mapping = {0: 'HC', 1: 'ST'}\n",
        "y_labels = y.map(label_mapping)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y_labels, palette='viridis')\n",
        "plt.title('PCA of EEG Features PC1 and PC2')\n",
        "plt.xlabel('Component 1')\n",
        "plt.ylabel('Component 2')\n",
        "plt.legend(title='Group', loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Plot PCA\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 3], hue=y, palette='viridis')\n",
        "plt.title('PCA of EEG Features PC1 and PC3')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 3')\n",
        "plt.legend(title='Group', loc='best')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ukdL4-IdMP5"
      },
      "source": [
        "#SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Fq2WKgt9LQw"
      },
      "outputs": [],
      "source": [
        "# Plot decision boundaries\n",
        "def plot_decision_boundaries(X, y, model, title):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
        "                         np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, levels=[-1, 0, 1], colors=['lightblue', 'lightcoral'])\n",
        "\n",
        "    # Plot data points with labels\n",
        "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', cmap=plt.cm.coolwarm)\n",
        "\n",
        "    # Create legend from custom artist/label lists\n",
        "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.coolwarm(0.), markersize=10, label='HC'),\n",
        "               plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.coolwarm(1.), markersize=10, label='ST')]\n",
        "    plt.legend(handles=handles)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('component 1')\n",
        "    plt.ylabel('component 2')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8S7PY3DdQqU"
      },
      "outputs": [],
      "source": [
        "# Separate features and labels\n",
        "X = Data_Frame.drop(columns=['Group', 'id','Group_Label'])\n",
        "y = Data_Frame['Group']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Perform PCA with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Variance explained by each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(f'Explained variance by each component: {explained_variance}')\n",
        "# print(f'Cumulative explained variance: {np.cumsum(explained_variance)}')\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the SVM model\n",
        "C_Config = 1\n",
        "gamma_value = 0.1\n",
        "# svm_rbf = SVC(C = C_Config , gamma = gamma_value, kernel='rbf')\n",
        "svm_rbf = SVC(C = C_Config, kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rbf)}\")\n",
        "print(classification_report(y_test, y_pred_rbf))\n",
        "\n",
        "print(f'y_test = {y_test}')\n",
        "print(f'y_pred = {y_pred_rbf}')\n",
        "\n",
        "# Plot decision boundary for training set\n",
        "plot_decision_boundaries(X_train, y_train, svm_rbf, 'SVM Model')\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "stratified_kf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(svm_rbf, X_pca, y, cv=stratified_kf)\n",
        "\n",
        "fold = 1\n",
        "for train_index, test_index in stratified_kf.split(X_pca, y):\n",
        "    X_train, X_test = X_pca[train_index], X_pca[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Create and train the SVM model\n",
        "    svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "    # Print classification report for the current fold\n",
        "    print(f'Fold {fold} Classification Report')\n",
        "    print(classification_report(y_test, y_pred_rbf))\n",
        "    fold += 1\n",
        "\n",
        "print(f'Cross-Validation Scores: {scores}')\n",
        "print(f'Mean Cross-Validation Score: {scores.mean()}')\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_rbf)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svm_rbf.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ROC plot"
      ],
      "metadata": {
        "id": "GYxfuXzOqHVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and labels\n",
        "X = Data_Frame.drop(columns=['Group', 'id'])\n",
        "y = Data_Frame['Group']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Perform PCA with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Variance explained by each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(f'Explained variance by each component: {explained_variance}')\n",
        "# print(f'Cumulative explained variance: {np.cumsum(explained_variance)}')\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the SVM model\n",
        "C_Config = 1\n",
        "svm_rbf = SVC(C=C_Config, kernel='rbf', probability=True)  # Enable probability estimates\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "y_prob_rbf = svm_rbf.predict_proba(X_test)[:, 1]  # Get the probability estimates for the positive class\n",
        "\n",
        "# Evaluate the model\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rbf)}\")\n",
        "print(classification_report(y_test, y_pred_rbf))\n",
        "\n",
        "print(f'y_test = {y_test.values}')\n",
        "print(f'y_pred = {y_pred_rbf}')\n",
        "\n",
        "# Plot decision boundary for training set\n",
        "plot_decision_boundaries(X_train, y_train, svm_rbf, 'SVM C = ' + str(C_Config))\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "stratified_kf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(svm_rbf, X_pca, y, cv=stratified_kf)\n",
        "print(f'Cross-Validation Scores: {scores}')\n",
        "print(f'Mean Cross-Validation Score: {scores.mean()}')\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_rbf)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svm_rbf.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve using Cross-Validation\n",
        "tprs = []\n",
        "aucs = []\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "for train_index, test_index in stratified_kf.split(X_pca, y):\n",
        "    X_train_cv, X_test_cv = X_pca[train_index], X_pca[test_index]\n",
        "    y_train_cv, y_test_cv = y[train_index], y[test_index]\n",
        "\n",
        "    svm_rbf.fit(X_train_cv, y_train_cv)\n",
        "    y_prob_rbf_cv = svm_rbf.predict_proba(X_test_cv)[:, 1]\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test_cv, y_prob_rbf_cv)\n",
        "    interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    auc = roc_auc_score(y_test_cv, y_prob_rbf_cv)\n",
        "    aucs.append(auc)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = np.mean(aucs)\n",
        "std_auc = np.std(aucs)\n",
        "\n",
        "# Smooth the ROC curve\n",
        "smooth_fpr = np.linspace(0, 1, 200)\n",
        "spl = make_interp_spline(mean_fpr, mean_tpr, k=3)\n",
        "smooth_tpr = spl(smooth_fpr)\n",
        "\n",
        "# Plot scatter with smooth lines\n",
        "plt.figure()\n",
        "# plt.scatter(mean_fpr, mean_tpr, color='blue', label='Data points')\n",
        "plt.plot(smooth_fpr, smooth_tpr, color='orange', lw=2, label=f'ROC curve (area = {mean_auc:.2f} ± {std_auc:.2f})')\n",
        "plt.fill_between(mean_fpr, np.maximum(mean_tpr - std_auc, 0), np.minimum(mean_tpr + std_auc, 1), color='orange', alpha=0.2)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "00u7e6YNpDT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Print True/False IDs"
      ],
      "metadata": {
        "id": "0R1UJ1zsqUf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and labels\n",
        "X = Data_Frame.drop(columns=['Group', 'id'])\n",
        "y = Data_Frame['Group']\n",
        "ids = Data_Frame['id']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Perform PCA with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(X_pca, y, ids, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set the parameters\n",
        "C_Config = 1\n",
        "gamma_value = 0.01\n",
        "\n",
        "# Create and train the SVM model\n",
        "svm_rbf = SVC(C=C_Config, kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rbf)}\")\n",
        "print(classification_report(y_test, y_pred_rbf))\n",
        "\n",
        "print(f'y_test = {y_test.values}')\n",
        "print(f'y_pred = {y_pred_rbf}')\n",
        "\n",
        "# Calculate Sensitivity (Recall)\n",
        "TP = cm[1, 1]\n",
        "FN = cm[1, 0]\n",
        "sensitivity = TP / (TP + FN)\n",
        "print(f'Sensitivity: {sensitivity}')\n",
        "\n",
        "# Calculate Specificity\n",
        "TN = cm[0, 0]\n",
        "FP = cm[0, 1]\n",
        "specificity = TN / (TN + FP)\n",
        "print(f'Specificity: {specificity}')\n",
        "\n",
        "# Confusion Matrix IDs\n",
        "tp_ids = ids_test[(y_test == 1) & (y_pred_rbf == 1)]\n",
        "tn_ids = ids_test[(y_test == 0) & (y_pred_rbf == 0)]\n",
        "fp_ids = ids_test[(y_test == 0) & (y_pred_rbf == 1)]\n",
        "fn_ids = ids_test[(y_test == 1) & (y_pred_rbf == 0)]\n",
        "\n",
        "print(f'True Positive IDs: {tp_ids.values}')\n",
        "print(f'True Negative IDs: {tn_ids.values}')\n",
        "print(f'False Positive IDs: {fp_ids.values}')\n",
        "print(f'False Negative IDs: {fn_ids.values}')\n",
        "\n",
        "# Plot decision boundary for training set\n",
        "plot_decision_boundaries(X_train, y_train, svm_rbf, 'SVM C = ' + str(C_Config))\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "stratified_kf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(svm_rbf, X_pca, y, cv=stratified_kf)\n",
        "print(f'Cross-Validation Scores: {scores}')\n",
        "print(f'Mean Cross-Validation Score: {scores.mean()}')\n"
      ],
      "metadata": {
        "id": "g5xmCJOTesVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#เพิ่ม id ในกราฟ"
      ],
      "metadata": {
        "id": "9vUnU0fPjgnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_decision_boundaries(X, y, model, title, ids):\n",
        "    plt.figure(figsize=(24, 20))\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
        "                         np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, levels=[-1, 0, 1], colors=['lightblue', 'lightcoral'])\n",
        "\n",
        "    # Plot data points with labels\n",
        "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', cmap=plt.cm.coolwarm)\n",
        "    for i, txt in enumerate(ids):\n",
        "        plt.annotate(txt, (X[i, 0], X[i, 1]), fontsize=15, ha='right')\n",
        "\n",
        "    # Create legend from custom artist/label lists\n",
        "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.coolwarm(0.), markersize=10, label='HC'),\n",
        "               plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.coolwarm(1.), markersize=10, label='ST')]\n",
        "    plt.legend(handles=handles)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('PCA component 1')\n",
        "    plt.ylabel('PCA component 2')\n",
        "    plt.show()\n",
        "\n",
        "# Separate features and labels\n",
        "X = Data_Frame.drop(columns=['Group', 'id'])\n",
        "y = Data_Frame['Group']\n",
        "ids = Data_Frame['id']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Perform PCA with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Variance explained by each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(f'Explained variance by each component: {explained_variance}')\n",
        "# print(f'Cumulative explained variance: {np.cumsum(explained_variance)}')\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(X_pca, y, ids, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the SVM model\n",
        "C_Config = 1\n",
        "svm_rbf = SVC(C=C_Config, kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rbf)}\")\n",
        "print(classification_report(y_test, y_pred_rbf))\n",
        "\n",
        "print(f'y_test = {y_test}')\n",
        "print(f'y_pred = {y_pred_rbf}')\n",
        "\n",
        "# Plot decision boundary for training set\n",
        "plot_decision_boundaries(X_train, y_train, svm_rbf, 'SVM C = ' + str(C_Config), ids_train)\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "stratified_kf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(svm_rbf, X_pca, y, cv=stratified_kf)\n",
        "print(f'Cross-Validation Scores: {scores}')\n",
        "print(f'Mean Cross-Validation Score: {scores.mean()}')"
      ],
      "metadata": {
        "id": "jjQzzpf3jIne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Grid Search"
      ],
      "metadata": {
        "id": "ueVhU7Y2ZWyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Separate features and labels\n",
        "X = Data_Frame.drop(columns=['Group', 'id'])\n",
        "y = Data_Frame['Group']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Perform PCA with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Variance explained by each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(f'Explained variance by each component: {explained_variance}')\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Create and train the SVM model using Grid Search\n",
        "grid_search = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "print(f'Best parameters found: {grid_search.best_params_}')\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred_rbf = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rbf)}\")\n",
        "print(classification_report(y_test, y_pred_rbf))\n",
        "\n",
        "print(f'y_test = {y_test}')\n",
        "print(f'y_pred = {y_pred_rbf}')\n",
        "\n",
        "# Plot decision boundary for training set\n",
        "plot_decision_boundaries(X_train, y_train, grid_search.best_estimator_, 'SVM with Grid Search')\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "stratified_kf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(grid_search.best_estimator_, X_pca, y, cv=stratified_kf)\n",
        "print(f'Cross-Validation Scores: {scores}')\n",
        "print(f'Mean Cross-Validation Score: {scores.mean()}')\n"
      ],
      "metadata": {
        "id": "r7TLRS8yZTF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Search"
      ],
      "metadata": {
        "id": "ZnqQyu44dNPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter distribution\n",
        "param_dist = {\n",
        "    'C': [0.1, 1, 10, 100, 1000],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Create and train the SVM model using Random Search\n",
        "random_search = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "print(f'Best parameters found: {random_search.best_params_}')\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred_rbf = random_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rbf)}\")\n",
        "print(classification_report(y_test, y_pred_rbf))\n",
        "\n",
        "print(f'y_test = {y_test}')\n",
        "print(f'y_pred = {y_pred_rbf}')\n",
        "\n",
        "# Plot decision boundary for training set\n",
        "plot_decision_boundaries(X_train, y_train, random_search.best_estimator_, 'SVM with Random Search')\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "stratified_kf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(random_search.best_estimator_, X_pca, y, cv=stratified_kf)\n",
        "print(f'Cross-Validation Scores: {scores}')\n",
        "print(f'Mean Cross-Validation Score: {scores.mean()}')"
      ],
      "metadata": {
        "id": "aofuqF6HZg2t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}